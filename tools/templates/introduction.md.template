Remote Persistent Memory Access (RPMA) uses Remote Direct Memory Access (RDMA) to provide easy to use interface for accessing Persistent Memory (PMem) on the remote system. This is a user-space library which may be used with all available RDMA implementations (InfiniBand&trade;, iWARP, RoCE, RoCEv2) from various providers as long as they support the standard RDMA user-space Linux interface namely the libiverbs library.

The RPMA-dedicated FIO engines are created as complementary pairs namely librpma\_apm\_client along with librpma\_apm\_server and librpma\_gpspm\_client along with librpma\_gpspm\_server. For the simplicity sake, both parts are implemented independently without any out of band communiation or daemons allowing to prepare the target memory for I/O requests. Before running the client engine on the initiator side the server engine has to be started. It prepares memory according to provided job file (either DRAM or PMem), registers it in the local RDMA-capable network interface (RNIC) and waits for preconfigured (via the job file) number of incoming client's connection. As the second one the client engine is started and establish a preconfigured number of connections with the server. After this setup the client engine starts executing I/O requests against the memory exposed on the server side.

The RPMA library and any application using it (including FIO with dedicated engines) should work on all flavours of RDMA transport but it is currently tested against RoCEv2.

For the sake of the measurements for this report, the FIO process is limited to execute and allocate all its buffers from a single NUMA node to avoid costly cross-NUMA UPI synchronizations.

Figure 1 shows a high-level schematic of the systems used for testing in the rest of this report. The set up consists of two individual systems (one used as initiator and one used as the target). The target is connected to the initiator system point-to-point using QSFP28 cables without any switches. The target system has six Intel&reg; Optane&trade; Persistent Memory devices connected to the respective NUMA node and one 100Gbps Mellanox ConnectX-5 NIC connected. The initiator system has one Mellanox ConnectX-5 100Gbps NIC connected directly to the target without any switch. From two ports available for Mellanox NICs only one is in use during the measurements.

![](./Figure_0{{hl_ext}})
**Figure 0**: High-Level RPMA performance testing setup
